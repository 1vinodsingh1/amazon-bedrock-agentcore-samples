{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d0b686-22f6-494b-952b-c97ee3bb0b60",
   "metadata": {},
   "source": [
    "# LangGraph with AgentCore Memory - Human in the Loop (Short term memory)\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates how to integrate Amazon Bedrock AgentCore Memory capabilities with LangGraph to create **human-in-the-loop** workflows. We'll focus on **short-term memory** persistence combined with the ability to interrupt agent execution for human intervention, creating sophisticated customer support scenarios with seamless handoffs.\n",
    "\n",
    "## Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                          |\n",
    "|:--------------------|:---------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Short Term Conversational                                                        |\n",
    "| Agent usecase       | Customer Support with Human Escalation                                          |\n",
    "| Agentic Framework   | Langgraph                                                                        |\n",
    "| LLM model           | Anthropic Claude Sonnet 3.7                                                      |\n",
    "| Tutorial components | AgentCore Short-term Memory, Langgraph Checkpointer, Human-in-the-Loop        |\n",
    "| Example complexity  | Beginner                                                                     |\n",
    "\n",
    "You'll learn to:\n",
    "- Create a memory checkpointer with AgentCore Memory for workflow persistence\n",
    "- Use LangGraph's interrupt mechanism for human-in-the-loop workflows\n",
    "- Implement tools that can pause execution for human intervention\n",
    "- Resume agent workflows after human input using LangGraph Commands\n",
    "- Manage complex customer support scenarios with seamless handoffs\n",
    "\n",
    "### Scenario Context\n",
    "\n",
    "In this example, we'll create a \"**Customer Support Agent**\" that can escalate complex issues to human supervisors. When the agent encounters situations requiring human expertise, it will pause execution, save the current state to AgentCore Memory, and wait for human intervention. The human supervisor can then provide guidance, and the agent will resume with the enhanced context.\n",
    "\n",
    "## Architecture\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- AWS account with appropriate permissions\n",
    "- AWS IAM role with appropriate permissions for AgentCore Memory\n",
    "- Access to Amazon Bedrock models\n",
    "\n",
    "### How the Integration Works\n",
    "\n",
    "The integration between LangGraph and AgentCore Memory for human-in-the-loop workflows involves:\n",
    "\n",
    "1. Using AgentCore Memory as a checkpointer backend for persistent state management\n",
    "2. Implementing interrupt mechanisms that pause execution at specific points\n",
    "3. Enabling human supervisors to resume workflows with additional context\n",
    "4. Maintaining conversation history and state across interruptions\n",
    "\n",
    "This approach creates support workflows where AI agents and human supervisors work together seamlessly.\n",
    "\n",
    "Let's get started by setting up our environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733879f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a3dfc-c14f-4a81-9c44-b44546d4196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Imports that enable human-in-the-loop implementation\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630de2fb-235a-4de5-a8e9-685a2babcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "# Import the AgentCoreMemorySaver that we will use as a checkpointer\n",
    "from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "\n",
    "logging.getLogger(\"support-agent\").setLevel(logging.INFO)\n",
    "region = os.getenv('AWS_REGION', 'us-west-2')\n",
    "\n",
    "logger = logging.getLogger(\"support-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accd612-ca34-450e-9001-2f90aea92218",
   "metadata": {},
   "source": [
    "## Step 1: Memory Creation\n",
    "In this section, we'll create a memory store using the AgentCore Memory SDK. This memory will serve as the backend for our LangGraph checkpointer and enable persistent human-in-the-loop workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e938cd3-32ce-4bc0-b215-c96ed2d16662",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_name = \"SupportAgent\"\n",
    "\n",
    "client = MemoryClient(region_name=region)\n",
    "memory = client.create_or_get_memory(name=memory_name)\n",
    "memory_id = memory[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f95ec-beb2-4007-a46c-730d0c156d18",
   "metadata": {},
   "source": [
    "### AgentCore Memory Configuration\n",
    "\n",
    "Now let's configure our AgentCore Memory checkpointer and initialize the LLM:\n",
    "\n",
    "- `memory_id` corresponds to our AgentCore Memory resource where checkpoints will be stored\n",
    "- `region` specifies the AWS region for our resources\n",
    "- `MODEL_ID` defines the Bedrock model that will power our LangGraph agent\n",
    "\n",
    "We will use the `memory_id` and any additional boto3 client keyword args (in our case, `region`) to instantiate our checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67b4ee-ff3f-4576-8072-8885c2a47e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "# Initialize checkpointer for state persistence\n",
    "checkpointer = AgentCoreMemorySaver(memory_id, region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f9b1e-fd88-4546-afdb-1168e3a88ff2",
   "metadata": {},
   "source": [
    "## Step 2: Human-in-the-Loop Tool\n",
    "Let's define the tools our support agent will use. Using the LangGraph `interrupt` type, we can interrupt the agent graph execution to give the chance for a human to intervene and respond to the query to continue execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb558a-8ee4-486c-92ab-3ff4485fdfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int):\n",
    "    \"\"\"Add two integers and return the result\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int):\n",
    "    \"\"\"Multiply two integers and return the result\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply, human_assistance]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30359ac1-90b1-4627-a4b0-203c1d598bf1",
   "metadata": {},
   "source": [
    "## Step 3: LangGraph Agent Implementation\n",
    "\n",
    "Now let's create our support agent using LangGraph's `create_react_agent` builder with our AgentCore Memory checkpointer and human-in-the-loop capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbc7289-6595-4b31-8472-47d2de00c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=\"You are a helpful assistant\",\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4b967-3c83-4f1c-ae22-d5ba97b7fbe1",
   "metadata": {},
   "source": [
    "## Step 4: Run the Support Agent\n",
    "We can now run the agent with our AgentCore Memory checkpointer and human-in-the-loop integration. For this example we will ask explicitly for user assistance. In reality, this could be triggered by several conditions, for example a safety flag may route a conversation to a human if certain keywords are used.\n",
    "\n",
    "### Configuration Setup\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. You can [read additional information here](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/).\n",
    "\n",
    "For the AgentCore Memory checkpointer (`AgentCoreMemorySaver`), we need to specify:\n",
    "- `thread_id`: Maps to AgentCore session_id (unique conversation thread)\n",
    "- `actor_id`: Maps to AgentCore actor_id (user, agent or other identifier)\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables as well but for the simple `create_react_agent`, only messages are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a6a6e-b16f-41dc-8f16-1d102f6debc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I would like to work with a customer service human agent.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"actor_id\": \"demo-notebook\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2af97-524a-4626-8dc4-85c5b677af90",
   "metadata": {},
   "source": [
    "### Workflow Interruption\n",
    "\n",
    "Notice how execution paused when the human assistance tool was called. Let's inspect the current state to see where the workflow stopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629af32-b660-4157-97ca-597b21ec66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31390c-0b1d-4b7f-9185-a28ee2bb79aa",
   "metadata": {},
   "source": [
    "### Human Supervisor intervention\n",
    "\n",
    "Now let's act as the human supervisor and provide assistance to resume the workflow using the LangGraph `Command` to send our response. The AgentCore Memory checkpointer has preserved the entire conversation state, that will allow us to resume the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf88072-ab8d-4e5f-a605-6d65e66f6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response = (\n",
    "    \"I'm sorry to hear that you are frustrated. Looking at the past conversation history, I can see that you've requested a refund. I've gone ahead and credited it to your account.\"\n",
    ")\n",
    "\n",
    "human_command = Command(resume={\"messages\": human_response})\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7748e-6bb5-42eb-96eb-1d60530c94a1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. How to create an AgentCore Memory resource for human-in-the-loop workflows\n",
    "2. Building a LangGraph agent with interrupt capabilities\n",
    "3. Implementing tools that can pause execution for human intervention\n",
    "4. Using the AgentCoreMemorySaver to persist workflow state during interruptions\n",
    "5. Resuming agent execution with human-provided context\n",
    "\n",
    "This integration showcases the power of combining LangGraph's human-in-the-loop capabilities with AgentCore Memory's robust state persistence to create sophisticated customer support workflows where AI agents and human supervisors work together seamlessly.\n",
    "\n",
    "The approach we've demonstrated can be extended to more complex scenarios, including multi-level escalations, specialized human expertise routing, and complex approval workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61378c15-f341-44e5-a669-fc07e3edcaf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:40:35.903279Z",
     "iopub.status.busy": "2025-10-02T17:40:35.902799Z",
     "iopub.status.idle": "2025-10-02T17:40:35.908250Z",
     "shell.execute_reply": "2025-10-02T17:40:35.907091Z",
     "shell.execute_reply.started": "2025-10-02T17:40:35.903230Z"
    }
   },
   "source": [
    "## Clean up\n",
    "Let's delete the memory to clean up the resources used in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751cd955-01c8-4fd9-a8ab-df0a04d8680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete_memory_and_wait(memory_id = memory_id, max_wait = 300, poll_interval =10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
