{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph with AgentCore Memory Hooks (Long-term Memory)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to integrate Amazon Bedrock AgentCore Memory capabilities with a conversational AI agent using LangGraph framework. We'll focus on **long-term memory** retention across multiple conversation sessions - allowing an agent to extract and recall user preferences, dietary restrictions, and contextual information from past interactions.\n",
    "\n",
    "## Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                          |\n",
    "|:--------------------|:---------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Long-term Conversational                                                        |\n",
    "| Agent usecase       | Nutrition Assistant                                                              |\n",
    "| Agentic Framework   | LangGraph                                                                        |\n",
    "| LLM model           | Anthropic Claude Sonnet 3.7                                                     |\n",
    "| Tutorial components | AgentCore Long-term Memory, Custom Memory Strategies, Pre/Post Model Hooks     |\n",
    "| Example complexity  | Intermediate                                                                     |\n",
    "\n",
    "You'll learn to:\n",
    "- Create AgentCore Memory with UserPreference custom-override strategy\n",
    "- Implement pre/post model hooks for automatic memory storage and retrieval\n",
    "- Build a nutrition assistant that remembers user preferences across sessions\n",
    "- Use semantic search to retrieve relevant user context\n",
    "- Configure custom memory extraction and consolidation prompts\n",
    "\n",
    "### Scenario Context\n",
    "\n",
    "In this example, we'll create a **Nutrition Assistant** that can remember user context across multiple conversations, including dietary restrictions, favorite foods, cooking preferences, and health goals. The agent will automatically extract and store user preferences from conversations, then retrieve relevant context for future interactions to provide personalized nutrition advice.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"architecture.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- AWS account with appropriate permissions\n",
    "- AWS IAM role with appropriate permissions for AgentCore Memory\n",
    "- Access to Amazon Bedrock models\n",
    "\n",
    "Let's get started by setting up our environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries from https://github.com/langchain-ai/langchain-aws\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Import LangGraph and LangChain components\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "import uuid\n",
    "\n",
    "\n",
    "region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "logging.getLogger(\"math-agent\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AgentCoreMemoryStore that we will use as a store\n",
    "from langgraph_checkpoint_aws import (\n",
    "    AgentCoreMemoryStore\n",
    ")\n",
    "\n",
    "# For this example, we will just use an InMemorySaver to save context.\n",
    "# In production, we highly recommend the AgentCoreMemorySaver as a checkpointer which works seamlessly alongside the memory store\n",
    "#from langgraph_checkpoint_aws import AgentCoreMemorySaver\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "from bedrock_agentcore.memory.constants import StrategyType\n",
    "\n",
    "from custom_memory_prompts import consolidation_prompt, extraction_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_name = \"NutritionAssistant\"\n",
    "client = MemoryClient(region_name=region)\n",
    "MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "memory = client.create_or_get_memory(\n",
    "    name=memory_name,\n",
    "    description=\"Nutrition assistant\",\n",
    "    memory_execution_role_arn=\"arn:aws:iam::YOUR_ACCOUNT:role/YOUR_ROLE\", # Please provide a role with a valid trust policy\n",
    "    strategies=[\n",
    "        {\n",
    "            StrategyType.CUSTOM.value: {\n",
    "                \"name\": \"NutritionPreferences\",\n",
    "                \"description\": \"Captures customer food preferences and behavior\",\n",
    "                \"namespaces\": [\"/{actorId}/preferences\"],\n",
    "                \"configuration\": {\n",
    "                    \"userPreferenceOverride\": {\n",
    "                        \"extraction\": {\n",
    "                            \"appendToPrompt\": extraction_prompt,\n",
    "                            \"modelId\": MODEL_ID,\n",
    "                        },\n",
    "                        \"consolidation\": {\n",
    "                            \"appendToPrompt\": consolidation_prompt,\n",
    "                            \"modelId\": MODEL_ID,\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "memory_id = memory[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Configuration Overview\n",
    "\n",
    "Our AgentCore Memory setup includes:\n",
    "\n",
    "- **Custom Strategy**: Extracts nutrition preferences from conversations\n",
    "- **Namespaces**: Organizes memories by user (`{actorId}/preferences`)\n",
    "- **Custom Prompts**: Specialized extraction and consolidation logic for food preferences\n",
    "- **Model Integration**: Uses Claude 3.7 Sonnet for memory processing\n",
    "\n",
    "The memory system will automatically process conversations to extract lasting user preferences while filtering out temporary or irrelevant information.\n",
    "\n",
    "## Step 3: Initialize Memory Store and LLM\n",
    "\n",
    "Now we'll initialize the AgentCore Memory Store and our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the store to enable long term memory saving and retrieval\n",
    "store = AgentCoreMemoryStore(memory_id=memory_id, region_name=region)\n",
    "\n",
    "# Initialize Bedrock LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Memory Hooks\n",
    "\n",
    "We'll create pre and post model hooks to automatically handle memory storage and retrieval:\n",
    "\n",
    "- **Pre-model hook**: Retrieves relevant user preferences (based on semantic search) and adds context before LLM invocation\n",
    "- **Post-model hook**: Saves the conversation messages for long-term memory extraction\n",
    "\n",
    "### How Memory Processing Works\n",
    "\n",
    "1. Messages are saved to AgentCore Memory with actor_id and session_id\n",
    "2. The custom strategy processes conversations to extract nutrition preferences\n",
    "3. Extracted preferences are stored in the `{actorId}/preferences` namespace\n",
    "4. Future conversations can search and retrieve relevant preferences for context\n",
    "\n",
    "**Note**: LangChain message types are converted under the hood by the store to AgentCore Memory message types so that they can be properly extracted to long term memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs pre-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the last human message we see before LLM invocation\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "    # Retrieve user preferences based on the last message and append to state\n",
    "    user_preferences_namespace = (actor_id, \"preferences\")\n",
    "    preferences = store.search(user_preferences_namespace, query=msg.content, limit=5)\n",
    "    \n",
    "    # Construct another AI message to add context before the current message\n",
    "    if preferences:\n",
    "        context_items = [pref.value for pref in preferences]\n",
    "        context_message = AIMessage(\n",
    "            content=f\"[User Context: {', '.join(str(item) for item in context_items)}]\"\n",
    "        )\n",
    "        # Insert the context message before the last human message\n",
    "        return {\"messages\": messages[:-1] + [context_message, messages[-1]]}\n",
    "    \n",
    "    return {\"llm_input_messages\": messages}\n",
    "\n",
    "def post_model_hook(state, config: RunnableConfig, *, store: BaseStore):\n",
    "    \"\"\"Hook that runs post-LLM invocation to save the latest human message\"\"\"\n",
    "    actor_id = config[\"configurable\"][\"actor_id\"]\n",
    "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
    "\n",
    "    # Saving the message to the actor and session combination that we get at runtime\n",
    "    namespace = (actor_id, thread_id)\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    # Save the LLMs response to AgentCore Memory\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"message\": msg})\n",
    "            break\n",
    "    \n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the LangGraph Agent\n",
    "\n",
    "Now we'll create our nutrition assistant agent using LangGraph's `create_react_agent` with our memory hooks integrated. The tool node will contain just our long term memory retrieval tool and the pre and post model hooks are specified as arguments.\n",
    "\n",
    "**Note**: for custom agent implementations the Store and tools can be configured to run as needed for any workflow following this pattern. Pre/post model hooks can be used, the whole conversation could be saved at the end, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_react_agent(\n",
    "    llm,\n",
    "    store=store,\n",
    "    tools=[], # No additional tools needed for this example\n",
    "    checkpointer=InMemorySaver(), # For conversation state management\n",
    "    pre_model_hook=pre_model_hook, # Retrieves user preferences before LLM call\n",
    "    post_model_hook=post_model_hook  # Saves conversation after LLM response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Agent Runtime\n",
    "\n",
    "We need to configure the agent with unique identifiers for the user and session. These IDs are crucial for memory organization and retrieval.\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables as well but for the simple `create_react_agent`, we only need messages.\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. For the `AgentCoreMemorySaver`, `thread_id` and `actor_id` must be set in the config. For instance, your AgentCore invocation endpoint could assign this based on the identity or user ID of the caller. You can read additional [documentation here](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_id = \"user-1\"\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-1\", # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": actor_id, # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Agent\n",
    "\n",
    "Let's test our nutrition assistant by having a conversation about food preferences. The agent will automatically extract and store user preferences for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to pretty print agent output while running\n",
    "def run_agent(query: str, config: RunnableConfig):\n",
    "    printed_ids = set()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                # Check if we've already printed this message\n",
    "                if id(msg) not in printed_ids:\n",
    "                    msg.pretty_print()\n",
    "                    printed_ids.add(id(msg))\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
    "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
    "and also improve the protein and vitamins I get?\n",
    "\"\"\"\n",
    "\n",
    "run_agent(prompt, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was stored?\n",
    "As you can see, the model does not yet have any insight into our preferences or dietary restrictions.\n",
    "\n",
    "For this implementation with pre/post model hooks, two messages were stored here. The first message from the user and the response from the AI model were both stored as conversational events in AgentCore Memory. It may take a few moments for the long term memories to be extracted, so retry after a few seconds if nothing is found the first try.\n",
    "\n",
    "These messages were then extracted to AgentCore long term memory in our fact and user preferences namespaces. In fact, we can check the store ourselves to verify what has been stored there so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search our user preferences namespace\n",
    "search_namespace = (actor_id, \"preferences\")\n",
    "result = store.search(search_namespace, query=\"food\", limit=3)\n",
    "print(f\"Preferences namespace result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent access to the store\n",
    "\n",
    "**Note** - since AgentCore memory processes these events in the background, it may take a few seconds for the memory to be extracted and embedded to long term memory retrieval.\n",
    "\n",
    "Great! Now we have seen that long term memories were extracted to our namespaces based on the earlier messages in the conversation.\n",
    "\n",
    "Now, let's start a new session and ask about recommendations for what to cook for dinner. The agent can use the store to access the long term memories that were extracted to make a recommendation that the user will be sure to like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"session-2\", # New session ID\n",
    "        \"actor_id\": actor_id, # Same actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "run_agent(\"Today's a new day, what should I make for dinner tonight?\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "As you can see, the agent received both pre-model hook context from the user preferences namespace search and was able to search on its own for long term memories in the fact namespace to create a comprehensive answer for the user.\n",
    "\n",
    "The AgentCoreMemoryStore is very flexible and can be implemented in a variety of ways, including pre/post model hooks or just tools themselves with store operations. Used alongside the AgentCoreMemorySaver for checkpointing, both full conversational state and long term insights can be combined to form a complex and intelligent agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
